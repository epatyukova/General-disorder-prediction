{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865fd6bd-ed8c-427a-ab7d-9d3d1349a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "import pytorch_lightning as L\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, matthews_corrcoef\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, StochasticWeightAveraging\n",
    "# from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from torchmetrics.functional import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from pymatgen.core.composition import Composition\n",
    "from crabnet.kingcrab import CrabNet\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CyclicLR, CosineAnnealingLR, StepLR\n",
    "\n",
    "from crabnet.utils.utils import (Lamb, Lookahead, RobustL1, BCEWithLogitsLoss,\n",
    "                         EDMDataset, get_edm, Scaler, DummyScaler, count_parameters)\n",
    "from crabnet.utils.get_compute_device import get_compute_device\n",
    "# from crabnet.utils.composition import _element_composition, get_sym_dict, parse_formula, CompositionError\n",
    "#from utils.optim import SWA\n",
    "\n",
    "data_type_np = np.float32\n",
    "data_type_torch = torch.float32\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "class CrabNetDataModule(L.LightningDataModule):\n",
    "    def __init__(self, train_file: str , \n",
    "                 val_file: str, \n",
    "                 test_file: str,\n",
    "                 n_elements ='infer', \n",
    "                 classification = False,\n",
    "                 elem_prop='mat2vec',\n",
    "                 batch_size = 2**10,\n",
    "                 scale = True,\n",
    "                 pin_memory = True):\n",
    "        super().__init__()\n",
    "        self.train_path = train_file\n",
    "        self.val_path = val_file\n",
    "        self.test_path = test_file\n",
    "        self.batch_size = batch_size\n",
    "        self.n_elements=n_elements\n",
    "        self.pin_memory = pin_memory\n",
    "        self.scale = scale\n",
    "        self.classification = classification\n",
    "        self.elem_prop=elem_prop\n",
    "\n",
    "    def prepare_data(self):\n",
    "        ### loading and encoding trianing data\n",
    "        if(re.search('.json', self.train_path )):\n",
    "            self.data_train=pd.read_json(self.train_path)\n",
    "        elif(re.search('.csv', self.train_path)):\n",
    "            self.data_train=pd.read_csv(self.train_path)\n",
    "\n",
    "        self.train_main_data = list(get_edm(self.data_train, elem_prop=self.elem_prop,\n",
    "                                      n_elements=self.n_elements,\n",
    "                                      inference=False,\n",
    "                                      verbose=True,\n",
    "                                      drop_unary=False,\n",
    "                                      scale=self.scale))\n",
    "        \n",
    "        self.train_len_data = len(self.train_main_data[0])\n",
    "        self.train_n_elements = self.train_main_data[0].shape[1]//2\n",
    "\n",
    "        print(f'loading data with up to {self.train_n_elements:0.0f} '\n",
    "              f'elements in the formula for training')\n",
    "        \n",
    "        ### loading and encoding validation data\n",
    "        if(re.search('.json', self.val_path )):\n",
    "            self.data_val=pd.read_json(self.val_path)\n",
    "        elif(re.search('.csv', self.val_path)):\n",
    "            self.data_val=pd.read_csv(self.val_path)\n",
    "        \n",
    "        self.val_main_data = list(get_edm(self.data_val, elem_prop=self.elem_prop,\n",
    "                                      n_elements=self.n_elements,\n",
    "                                      inference=True,\n",
    "                                      verbose=True,\n",
    "                                      drop_unary=False,\n",
    "                                      scale=self.scale))\n",
    "        \n",
    "        self.val_len_data = len(self.val_main_data[0])\n",
    "        self.val_n_elements = self.val_main_data[0].shape[1]//2\n",
    "\n",
    "        print(f'loading data with up to {self.val_n_elements:0.0f} '\n",
    "              f'elements in the formula for validation')\n",
    "        \n",
    "        ### loading and encoding testing data\n",
    "        if(re.search('.json', self.test_path )):\n",
    "            self.data_test=pd.read_json(self.test_path)\n",
    "        elif(re.search('.csv', self.test_path)):\n",
    "            self.data_test=pd.read_csv(self.test_path)\n",
    "        \n",
    "        self.test_main_data = list(get_edm(self.data_test, elem_prop=self.elem_prop,\n",
    "                                      n_elements=self.n_elements,\n",
    "                                      inference=True,\n",
    "                                      verbose=True,\n",
    "                                      drop_unary=False,\n",
    "                                      scale=self.scale))\n",
    "        \n",
    "        self.test_len_data = len(self.test_main_data[0])\n",
    "        self.test_n_elements = self.test_main_data[0].shape[1]//2\n",
    "\n",
    "        print(f'loading data with up to {self.test_n_elements:0.0f} '\n",
    "              f'elements in the formula for testing')\n",
    "\n",
    "        self.train_dataset = EDMDataset(self.train_main_data, self.train_n_elements)\n",
    "        self.val_dataset = EDMDataset(self.val_main_data, self.val_n_elements)\n",
    "        self.test_dataset = EDMDataset(self.test_main_data, self.test_n_elements)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=self.pin_memory, shuffle=True)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                        pin_memory=self.pin_memory, shuffle=False)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.test_len_data,\n",
    "                        pin_memory=self.pin_memory, shuffle=False)\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.test_len_data,\n",
    "                        pin_memory=self.pin_memory, shuffle=False)\n",
    "\n",
    "\n",
    "class CrabNetLightning(L.LightningModule):\n",
    "    def __init__(self, **config):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = CrabNet(out_dims=config['out_dims'],\n",
    "                             d_model=config['d_model'],\n",
    "                             N=config['N'],\n",
    "                             heads=config['heads'])\n",
    "        print('\\nModel architecture: out_dims, d_model, N, heads')\n",
    "        print(f'{self.model.out_dims}, {self.model.d_model}, '\n",
    "                  f'{self.model.N}, {self.model.heads}')\n",
    "        print(f'Model size: {count_parameters(self.model)} parameters\\n')\n",
    "\n",
    "        ### here we define some important parameters\n",
    "        self.fudge=config['fudge']\n",
    "        self.batch_size=config['batch_size']\n",
    "        self.classification = config['classification']\n",
    "        self.base_lr=config['base_lr']\n",
    "        self.max_lr=config['max_lr']\n",
    "        ### here we also need to initialise scaler based on training data\n",
    "        if(re.search('.json', config['train_path'] )):\n",
    "            train_data=pd.read_json(config['train_path'])\n",
    "        elif(re.search('.csv', config['train_path'])):\n",
    "            train_data=pd.read_csv(config['train_path'])\n",
    "        \n",
    "        y=train_data['target'].values\n",
    "        self.step_size = len(y)\n",
    "        if self.classification:\n",
    "            self.scaler = DummyScaler(y)\n",
    "        else:\n",
    "            self.scaler = Scaler(y)\n",
    "        ### we also define loss function based on task\n",
    "        if self.classification:\n",
    "            if(np.sum(y)>0):\n",
    "                self.weight=torch.tensor(((len(y)-np.sum(y))/np.sum(y))).cuda()\n",
    "            print(\"Using BCE loss for classification task\")\n",
    "            self.criterion = BCEWithLogitsLoss\n",
    "        else:\n",
    "            print(\"Using RobustL1 loss for regression task\")\n",
    "            self.criterion = RobustL1\n",
    "\n",
    "    def forward(self, src, frac):\n",
    "        out=self.model(src, frac)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        base_optim = Lamb(params=self.model.parameters(),lr=0.001)\n",
    "        optimizer = Lookahead(base_optimizer=base_optim)\n",
    "        lr_scheduler = CyclicLR(optimizer,\n",
    "                                base_lr=self.base_lr,\n",
    "                                max_lr=self.max_lr,\n",
    "                                cycle_momentum=False,\n",
    "                                step_size_up=self.step_size)\n",
    "        # lr_scheduler=StepLR(optimizer,\n",
    "        #                     step_size=3,\n",
    "        #                     gamma=0.5)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, formula = batch\n",
    "        y = self.scaler.scale(y)\n",
    "        src, frac = X.squeeze(-1).chunk(2, dim=1)\n",
    "        frac = frac * (1 + (torch.randn_like(frac))*self.fudge)\n",
    "        frac = torch.clamp(frac, 0, 1)\n",
    "        frac[src == 0] = 0\n",
    "        frac = frac / frac.sum(dim=1).unsqueeze(1).repeat(1, frac.shape[-1])\n",
    "        \n",
    "        output = self(src, frac)\n",
    "        prediction, uncertainty = output.chunk(2, dim=-1)\n",
    "        loss = self.criterion(prediction.view(-1),\n",
    "                              uncertainty.view(-1),\n",
    "                              y.view(-1), self.weight)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        uncertainty = torch.exp(uncertainty) * self.scaler.std\n",
    "        prediction = self.scaler.unscale(prediction)\n",
    "        if self.classification:\n",
    "            prediction = torch.sigmoid(prediction)\n",
    "            y_pred = prediction.view(-1).detach().cpu().numpy() > 0.5\n",
    "            acc=balanced_accuracy_score(y.view(-1).detach().cpu().numpy(),y_pred)\n",
    "            f1=f1_score(y.view(-1).detach().cpu().numpy(),y_pred,average='weighted')\n",
    "            mc=matthews_corrcoef(y.view(-1).detach().cpu().numpy(),y_pred)\n",
    "            \n",
    "            self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "            self.log(\"train_f1\", f1, on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "            self.log(\"train_mc\", mc, on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        else:\n",
    "            mse = mean_squared_error(prediction.view(-1),y.view(-1))\n",
    "            mae = mean_absolute_error(prediction.view(-1),y.view(-1))\n",
    "            self.log(\"train_mse\", mse, on_step=True, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "            self.log(\"train_mae\", mae, on_step=True, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y, formula = batch\n",
    "        y = self.scaler.scale(y)\n",
    "        src, frac = X.squeeze(-1).chunk(2, dim=1)\n",
    "        frac = frac * (1 + (torch.randn_like(frac))*self.fudge)\n",
    "        frac = torch.clamp(frac, 0, 1)\n",
    "        frac[src == 0] = 0\n",
    "        frac = frac / frac.sum(dim=1).unsqueeze(1).repeat(1, frac.shape[-1])\n",
    "        \n",
    "        output = self(src, frac)\n",
    "        prediction, uncertainty = output.chunk(2, dim=-1)\n",
    "        val_loss = self.criterion(prediction.view(-1),\n",
    "                              uncertainty.view(-1),\n",
    "                              y.view(-1), self.weight)\n",
    "        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        uncertainty = torch.exp(uncertainty) * self.scaler.std\n",
    "        prediction = self.scaler.unscale(prediction)\n",
    "        if self.classification:\n",
    "            prediction = torch.sigmoid(prediction)\n",
    "            y_pred = prediction.view(-1).detach().cpu().numpy() > 0.5\n",
    "            acc=balanced_accuracy_score(y.view(-1).detach().cpu().numpy(),y_pred)\n",
    "            f1=f1_score(y.view(-1).detach().cpu().numpy(),y_pred,average='weighted')\n",
    "            mc=matthews_corrcoef(y.view(-1).detach().cpu().numpy(),y_pred)\n",
    "            \n",
    "            self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "            self.log(\"val_f1\", f1, on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "            self.log(\"val_mc\", mc, on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        else:\n",
    "            mse = mean_squared_error(prediction.view(-1),y.view(-1))\n",
    "            mae = mean_absolute_error(prediction.view(-1),y.view(-1))\n",
    "            self.log(\"val_mse\", mse, on_step=True, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "            self.log(\"val_mae\", mae, on_step=True, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        return val_loss\n",
    "     \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y, formula = batch\n",
    "        y = self.scaler.scale(y)\n",
    "        src, frac = X.squeeze(-1).chunk(2, dim=1)\n",
    "        frac = frac * (1 + (torch.randn_like(frac))*self.fudge)\n",
    "        frac = torch.clamp(frac, 0, 1)\n",
    "        frac[src == 0] = 0\n",
    "        frac = frac / frac.sum(dim=1).unsqueeze(1).repeat(1, frac.shape[-1])\n",
    "        \n",
    "        output = self(src, frac)\n",
    "        prediction, uncertainty = output.chunk(2, dim=-1)\n",
    "        uncertainty = torch.exp(uncertainty) * self.scaler.std\n",
    "        prediction = self.scaler.unscale(prediction)\n",
    "        if self.classification:\n",
    "            prediction = torch.sigmoid(prediction)\n",
    "            y_pred = prediction.view(-1).detach().cpu().numpy() > 0.5\n",
    "            acc=balanced_accuracy_score(y.view(-1).detach().cpu().numpy(),y_pred)\n",
    "            f1=f1_score(y.view(-1).detach().cpu().numpy(),y_pred,average='weighted')\n",
    "            mc=matthews_corrcoef(y.view(-1).detach().cpu().numpy(),y_pred)\n",
    "            \n",
    "            self.log(\"test_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "            self.log(\"test_f1\", f1, on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "            self.log(\"test_mc\", mc, on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        else:\n",
    "            mse = mean_squared_error(prediction.view(-1),y.view(-1))\n",
    "            mae = mean_absolute_error(prediction.view(-1),y.view(-1))\n",
    "            self.log(\"test_mse\", mse, on_step=True, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "            self.log(\"test_mae\", mae, on_step=True, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        return \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        X, y, formula = batch\n",
    "        y = self.scaler.scale(y)\n",
    "        src, frac = X.squeeze(-1).chunk(2, dim=1)\n",
    "        frac = frac * (1 + (torch.randn_like(frac))*self.fudge)\n",
    "        frac = torch.clamp(frac, 0, 1)\n",
    "        frac[src == 0] = 0\n",
    "        frac = frac / frac.sum(dim=1).unsqueeze(1).repeat(1, frac.shape[-1])\n",
    "        \n",
    "        output = self(src, frac)\n",
    "        prediction, uncertainty = output.chunk(2, dim=-1)\n",
    "        uncertainty = torch.exp(uncertainty) * self.scaler.std\n",
    "        prediction = self.scaler.unscale(prediction)\n",
    "        if self.classification:\n",
    "            prediction = torch.sigmoid(prediction)\n",
    "\n",
    "        y_pred = prediction.view(-1).detach().cpu().numpy() > 0.5\n",
    "        acc=balanced_accuracy_score(y.view(-1).detach().cpu().numpy(),y_pred)\n",
    "        f1=f1_score(y.view(-1).detach().cpu().numpy(),y_pred,average='weighted')\n",
    "        mc=matthews_corrcoef(y.view(-1).detach().cpu().numpy(),y_pred)\n",
    "        self.log(\"predict_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        self.log(\"predict_f1\", f1, on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        self.log(\"predict_mc\", mc, on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        return formula, y_pred, prediction, uncertainty\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960839e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('crabnet/crabnet_config.json','r') as f:\n",
    "        config=json.load(f)\n",
    "\n",
    "L.seed_everything(config['random_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba25ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model architecture: out_dims, d_model, N, heads\n",
      "3, 512, 3, 4\n",
      "Model size: 11987206 parameters\n",
      "\n",
      "Using BCE loss for classification task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Generating EDM: 100%|██████████| 75091/75091 [00:00<00:00, 207925.06formulae/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating EDM: 100%|██████████| 8344/8344 [00:00<00:00, 208416.34formulae/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating EDM: 100%|██████████| 20859/20859 [00:00<00:00, 210505.90formulae/s]\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patykova\\.conda\\envs\\roost\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory crabnet_models/crabnet_trained_models/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | CrabNet | 12.0 M\n",
      "----------------------------------\n",
      "12.0 M    Trainable params\n",
      "23.8 K    Non-trainable params\n",
      "12.0 M    Total params\n",
      "48.044    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a0051cde1b42b29384003f05eab260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patykova\\.conda\\envs\\roost\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\patykova\\.conda\\envs\\roost\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21e01c31c784e2a8e33bbfe0e52205f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Swapping scheduler `CyclicLR` for `SWALR`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74530ba4aba04043ac61a7be1cbc93cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff22d714fd60416d81ddeffc11f041f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d1f957ac2841aab63a9c260e22507a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb0e9f78cd14296be10c517f926cc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8f2ab97fb54e60ac4692a6d1a7d399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e117a3997ff44190b1032f0640bed83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ce536abc324c31a7e6c629fc97a4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0552984cc42b4f6e8e83575e0607536c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5968febe9e0046e1b798bcd21d5b6d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24f159775bc4721aee74167a2f1fef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "Generating EDM: 100%|██████████| 75091/75091 [00:00<00:00, 212528.78formulae/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating EDM: 100%|██████████| 8344/8344 [00:00<00:00, 205074.96formulae/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating EDM: 100%|██████████| 20859/20859 [00:00<00:00, 202331.59formulae/s]\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at crabnet_models/crabnet_trained_models/disorder-epoch=08-val_acc=0.89.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at crabnet_models/crabnet_trained_models/disorder-epoch=08-val_acc=0.89.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patykova\\.conda\\envs\\roost\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca916f8dea704f3ab49158b8c53bfbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.8879008745264143\n",
      "         test_f1            0.8783106564072712\n",
      "         test_mc            0.7603236724744891\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.8879008745264143,\n",
       "  'test_f1': 0.8783106564072712,\n",
       "  'test_mc': 0.7603236724744891}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CrabNetLightning(**config)\n",
    "# wandb_logger = WandbLogger(project=\"Crabnet-global-disorder-new\", config=config, log_model=\"all\")\n",
    "trainer = Trainer(max_epochs=10,accelerator='gpu', devices=1, \n",
    "                      callbacks=[StochasticWeightAveraging(swa_epoch_start=config['swa_epoch_start'],swa_lrs=config['swa_lrs']),\n",
    "                                EarlyStopping(monitor='val_loss', patience=config['patience']), ModelCheckpoint(monitor='val_acc', mode=\"max\", \n",
    "                                dirpath='crabnet_models/crabnet_trained_models/', filename='disorder-{epoch:02d}-{val_acc:.2f}')])\n",
    "disorder_data = CrabNetDataModule(config['train_path'],\n",
    "                                   config['val_path'],\n",
    "                                   config['test_path'],\n",
    "                                   classification = config['classification'])\n",
    "trainer.fit(model, datamodule=disorder_data)\n",
    "trainer.test(ckpt_path='best',datamodule=disorder_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08e8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in disorder_data.predict_dataloader():\n",
    "    X, y_true, formula = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "009ecb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating EDM: 100%|██████████| 75091/75091 [00:00<00:00, 204244.68formulae/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating EDM: 100%|██████████| 8344/8344 [00:00<00:00, 198486.13formulae/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating EDM: 100%|██████████| 20859/20859 [00:00<00:00, 208400.89formulae/s]\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at crabnet_models/crabnet_trained_models/disorder-epoch=08-val_acc=0.89.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from checkpoint at crabnet_models/crabnet_trained_models/disorder-epoch=08-val_acc=0.89.ckpt\n",
      "c:\\Users\\patykova\\.conda\\envs\\roost\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5561b5f65ca04f218dfad511fa061098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.8879008745264143\n",
      "         test_f1            0.8783106564072712\n",
      "         test_mc            0.7603236724744891\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.8879008745264143,\n",
       "  'test_f1': 0.8783106564072712,\n",
       "  'test_mc': 0.7603236724744891}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(ckpt_path='best', datamodule=disorder_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d63e676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating EDM: 100%|██████████| 75091/75091 [00:00<00:00, 211632.95formulae/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating EDM: 100%|██████████| 8344/8344 [00:00<00:00, 198487.25formulae/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating EDM: 100%|██████████| 20859/20859 [00:00<00:00, 206336.60formulae/s]\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at crabnet_models/crabnet_trained_models/disorder-epoch=08-val_acc=0.89.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data with up to 16 elements in the formula for testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from checkpoint at crabnet_models/crabnet_trained_models/disorder-epoch=08-val_acc=0.89.ckpt\n",
      "c:\\Users\\patykova\\.conda\\envs\\roost\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02c3c7d30874800b6c406c3f7b8ad31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 74it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formula, prediction, uncertainty=trainer.predict(ckpt_path='best', datamodule=disorder_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3368e5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [0.9639],\n",
       "        [0.0169],\n",
       "        ...,\n",
       "        [0.9998],\n",
       "        [0.9998],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19cbdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = prediction.view(-1).detach().cpu().numpy() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d37da61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True,  True, False, ...,  True,  True,  True]),\n",
       " tensor([1., 1., 0.,  ..., 1., 1., 1.]),\n",
       " tensor([[1.0000],\n",
       "         [0.9639],\n",
       "         [0.0169],\n",
       "         ...,\n",
       "         [0.9998],\n",
       "         [0.9998],\n",
       "         [1.0000]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred,y_true,prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db711242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8879008745264143"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "413b7faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8783106564072712"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59d74a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7603236724744891"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c388f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507252526634921"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(**config):\n",
    "    model = CrabNetLightning(**config)\n",
    "    wandb_logger = WandbLogger(project=\"Crabnet-global-disorder-new\", config=config, log_model=\"all\")\n",
    "    trainer = Trainer(max_epochs=100,accelerator='gpu', devices=1, logger=wandb_logger,\n",
    "                      callbacks=[StochasticWeightAveraging(swa_epoch_start=config['swa_epoch_start'],swa_lrs=config['swa_lrs']),\n",
    "                                EarlyStopping(monitor='val_loss', patience=config['patience']), ModelCheckpoint(monitor='val_acc', mode=\"max\", \n",
    "                                dirpath='crabnet_models/crabnet_trained_models/', filename='disorder-{epoch:02d}-{val_acc:.2f}')])\n",
    "    disorder_data = CrabNetDataModule(config['train_path'],\n",
    "                                   config['val_path'],\n",
    "                                   config['test_path'],\n",
    "                                   classification = config['classification'])\n",
    "    trainer.fit(model, datamodule=disorder_data)\n",
    "    trainer.test(ckpt_path='best',datamodule=disorder_data)\n",
    "\n",
    "    formula, prediction, uncertainty=trainer.predict(ckpt_path='best', datamodule=disorder_data)\n",
    "    metrics={}\n",
    "    metrics['acc']=balanced_accuracy_score(y_true,y_pred)\n",
    "    metrics['f1']=f1_score(y_true,y_pred,average='weighted')\n",
    "    metrics['precision']=precision_score(y_true,y_pred)\n",
    "    metrics['recall']=recall_score(y_true,y_pred)\n",
    "    metrics['mc']=matthews_corrcoef(y_true,y_pred)\n",
    "    metrics['roc_auc']=roc_auc_score(y_true,prediction)\n",
    "    metrics['conf_matrix']=confusion_matrix(y_true,y_pred)\n",
    "    pred_matrix={}\n",
    "    pred_matrix['y_true']=y_true\n",
    "    pred_matrix['y_score']=prediction.detach().numpy()\n",
    "    pred_matrix['y_true']=y_pred\n",
    "   \n",
    "    wandb.log(metrics)\n",
    "    wandb.log(pred_matrix)\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207459ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    wandb.init(project=\"Crabnet-global-disorder-ne\")\n",
    "    wandb.login(key='')\n",
    "\n",
    "    with open('crabnet/crabnet_config.json','r') as f:\n",
    "        config=json.load(f)\n",
    "\n",
    "    L.seed_everything(config['random_seed'])\n",
    "    main(**config)\n",
    "\n",
    "    wandb.finish()\n",
    "    # print('Start sweeping with different parameters for RF...')\n",
    "\n",
    "    # wandb.login(key='')\n",
    "\n",
    "    # sweep_config = {\n",
    "    # 'method': 'random',\n",
    "    # 'parameters': {'n_estimators': {'values': [50, 100, 150, 200]},\n",
    "    #                'class_weight': {'values':['balanced', 'balanced_subsample']},\n",
    "    #                'criterion': {'values': ['gini', 'entropy', 'log_loss']}\n",
    "    # }\n",
    "    # }\n",
    "\n",
    "    # sweep_id = wandb.sweep(sweep=sweep_config, project=\"RF-disorder-prediction-global-disorder\")\n",
    "\n",
    "    # wandb.agent(sweep_id, function=main, count=10)\n",
    "\n",
    "    # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e8196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54b9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26095c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c11c2-cb06-40e8-aba2-871884d01788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
